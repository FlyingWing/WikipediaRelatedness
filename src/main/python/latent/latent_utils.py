import os
import json
import pandas as pd

from gensim import utils
from pyhocon import ConfigFactory


def extract_json_pages(filename, filter_namespaces=False, keep_wikids=None):
    with utils.smart_open(filename) as fin:
        for line in fin:
            document = json.loads(line.strip())

            title = document['wikiTitle']
            wiki_id = str(document['wikiId'])

            try:
                if keep_wikids is None or int(wiki_id) in keep_wikids:

                    text = ' '.join(document['sentences'])
                    yield title, text, wiki_id

            except Exception:
                continue


            # yield title, text, wiki_id



def absolute_path(filename):
    '''
    :param filename:
    :return: Absolute path of filename
    '''
    WORKING_DIR = os.path.dirname(os.path.abspath(__file__))
    return os.path.join(WORKING_DIR, filename)




def dataset_paths():
    reference = absolute_path('../../resources/reference.conf')

    conf =  ConfigFactory.parse_file(reference)
    corrdir = conf.get_string('wikipediarelatedness.benchmark.correlation')

    datapaths = []
    for dataname in os.listdir(corrdir):
        datapaths.append( os.path.join(corrdir, dataname) )

    return datapaths



def configuration():
    """
    :return: Dictionary of the reference.conf file.
    """
    reference = absolute_path('../../resources/reference.conf')
    return ConfigFactory.parse_file(reference).get('wikipediarelatedness')



def get_wiki_ids():
    '''
    :return:  set of WikiIDs which belongs to dataset files.
    '''

    conf = configuration()

    # WikiSim
    filename = conf.get_string('dataset.wikisim.wat')
    df = pd.read_csv(filename, header=None)
    df.columns =  ['src_word', 'src_wiki_id', 'src_wiki_title', 'dst_word', 'dst_wiki_id', 'dst_wiki_title', 'rel']
    wiki_ids = df['src_wiki_id'].tolist() + df['dst_wiki_id'].tolist()

    # WiRe
    for wire_name in ['salient_salient', 'nonsalient_salient', 'nonsalient_nonsalient']:
        filename  = conf.get_string('dataset.wire.' + wire_name)
        df = pd.read_csv(filename)

        wiki_ids += df['srcWikiID'].tolist()
        wiki_ids += df['dstWikiID'].tolist()

    return set(wiki_ids)



LEMMING = True  # whether using lemmatization or not


conf = configuration()
wikiconf = conf.get('wikipedia')


WIKI_CORPUS = wikiconf.get('corpus')
WIKI_LINKS = wikiconf.get('links')


#
# Gensim Pre-processing

# Warning: Refactored
#GENSIM_DIR = absolute_path('../../../../data/processing/wikipedia/latent/gensim')
WIKI_STATS = 'wiki_corpus'  # name of the beginnig of each file generated by Genism
#WIKI_FILENAME = os.path.join(GENSIM_DIR, WIKI_STATS + '/' + WIKI_STATS + '_')

WIKI_FILENAME = os.path.join( wikiconf.get('latent.lda.corpus'), WIKI_STATS + '_' )


#
# Gensim LDA Topic Modeling

WIKI_LDA_DIR = wikiconf.get('latent.lda.topics')
# WIKI_LDA_MODEL = os.path.join(WIKI_LDA_DIR, 'lda_model')
# WIKI_LDA_DOCS = os.path.join(WIKI_LDA_DIR, 'lda_wiki_docs.gz')


def LDA_MODEL_DIR(num_topics):
    return os.path.join(WIKI_LDA_DIR, str(num_topics) + '/model')

def LDA_MODEL_FILENAME(num_topics):
    return os.path.join( LDA_MODEL_DIR(num_topics) , 'wiki_lda_model')

#
# To be refactored with reference.conf

WIKI_SVD_DIR = absolute_path('../../../../data/processing/wikipedia/latent/svd')

LAPLACIAN_PINV_DIR = absolute_path('../../../../data/processing/wikipedia/latent/pinv')